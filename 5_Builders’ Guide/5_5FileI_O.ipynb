{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.5 读写文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.1 加载和保存张量\n",
    "\n",
    "对于单个张量，我们可以直接调用`load`和`save`函数分别读写它们。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "x = torch.arange(4)\n",
    "torch.save(x, 'data/x-file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们现在可以将存储在文件中的数据读回内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = torch.load('data/x-file')\n",
    "x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以存储一个张量列表，然后把它们读回内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3]), tensor([0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.zeros(4)\n",
    "torch.save([x, y], 'data/xy-file')\n",
    "x2, y2 = torch.load('data/xy-file')\n",
    "x2, y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们甚至可以写入或读取从字符串映射到张量的字典。当我们需要读取或写入模型中的所有权重时，这很方便。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([0, 1, 2, 3]), 'y': tensor([0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydict = {'x':x, 'y':y}\n",
    "torch.save(mydict, 'data/mydict')\n",
    "mydict2 = torch.load('data/mydict')\n",
    "mydict2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 加载和保存模型参数\n",
    "\n",
    "保存单个权重向量(或者其他张量确实有用)，但是如果我们想保存整个模型，并在以后加载它们，单独保存每个向量则会变得很麻烦。因此，深度学习框架提供了内置函数来保存和加载整个网络。需要注意的一个细节是，这将保存模型的参数而不是整个模型。例如，如果我们有一个3层多层感知机，我们需要单独指定架构，因为模型本身可以包含任意代码，所以模型很难序列化。因此，为了恢复模型，我们需要用代码生成架构，再从磁盘加载数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('hidden.weight',\n",
       "              tensor([[ 0.1414,  0.1141,  0.1411,  ..., -0.1938, -0.0972,  0.1779],\n",
       "                      [ 0.1290,  0.2083,  0.1019,  ..., -0.0998,  0.0869, -0.1471],\n",
       "                      [ 0.1041, -0.0460,  0.2004,  ..., -0.0145,  0.2082, -0.1052],\n",
       "                      ...,\n",
       "                      [-0.1081,  0.2127,  0.1712,  ...,  0.1011, -0.1256, -0.1603],\n",
       "                      [ 0.2049, -0.1692, -0.2016,  ..., -0.0199,  0.2208,  0.0893],\n",
       "                      [-0.1357,  0.0959, -0.0024,  ..., -0.0014, -0.0049, -0.0782]])),\n",
       "             ('hidden.bias',\n",
       "              tensor([ 0.0437,  0.0841, -0.0536, -0.2051, -0.1086,  0.1979,  0.1654, -0.0346,\n",
       "                      -0.1985, -0.0431,  0.0850,  0.0443, -0.2171,  0.1685,  0.0631,  0.0295,\n",
       "                       0.1304,  0.1132, -0.1349,  0.0810, -0.1759,  0.1449,  0.0276, -0.0083,\n",
       "                       0.1072, -0.1034, -0.1048,  0.0414, -0.0287, -0.2133,  0.1733, -0.0046,\n",
       "                      -0.0378, -0.0499,  0.0577, -0.0286,  0.2005, -0.0867,  0.0637, -0.0222,\n",
       "                      -0.0020, -0.0855,  0.1005,  0.1562,  0.1404,  0.0958, -0.1356,  0.1315,\n",
       "                       0.1949,  0.1450,  0.0310,  0.0820,  0.0410,  0.1851, -0.1997, -0.0430,\n",
       "                      -0.0371, -0.0603, -0.1750,  0.1398, -0.0318, -0.1276, -0.0933,  0.1969,\n",
       "                       0.0103, -0.1864, -0.0752, -0.2141, -0.1681,  0.1007,  0.1034, -0.0283,\n",
       "                      -0.0762,  0.0723,  0.0381, -0.1069, -0.0367, -0.1376,  0.1508, -0.2176,\n",
       "                      -0.1485,  0.0316, -0.1033,  0.1949,  0.0929,  0.0844,  0.2175, -0.1596,\n",
       "                      -0.0823, -0.0957, -0.0164,  0.1863,  0.0914, -0.2235,  0.0674,  0.1261,\n",
       "                      -0.0940, -0.1420,  0.1420,  0.1259,  0.1158, -0.0164, -0.0453, -0.1461,\n",
       "                      -0.1524,  0.2146, -0.0817,  0.1838,  0.0489,  0.0192, -0.1781, -0.1837,\n",
       "                      -0.0425, -0.1196, -0.2131,  0.0146, -0.1600, -0.1889,  0.1204,  0.2171,\n",
       "                      -0.1332, -0.1782, -0.1685, -0.2129,  0.1664,  0.1660, -0.1478,  0.2170,\n",
       "                       0.1169,  0.0023,  0.1537, -0.2225,  0.0776, -0.0825,  0.1439,  0.1924,\n",
       "                       0.1233,  0.0685,  0.0363,  0.1406,  0.0177,  0.1104, -0.2128,  0.2222,\n",
       "                       0.0880, -0.1817, -0.1520, -0.1374, -0.0360, -0.1771, -0.1830,  0.0447,\n",
       "                      -0.1727,  0.1286, -0.1620, -0.0199, -0.1661,  0.0971,  0.0324, -0.1265,\n",
       "                       0.0189, -0.2175, -0.1850,  0.0216,  0.0460, -0.1984, -0.1685, -0.2171,\n",
       "                       0.1771,  0.0682,  0.0165,  0.1139,  0.0608,  0.0556, -0.1521, -0.0497,\n",
       "                      -0.1567,  0.0789,  0.0543, -0.0347,  0.1029, -0.1838, -0.1442,  0.1692,\n",
       "                      -0.0327,  0.1152,  0.0173, -0.1502, -0.1145, -0.0508,  0.2197,  0.0978,\n",
       "                       0.1737, -0.1133,  0.0869, -0.1895,  0.1442, -0.1006,  0.1887,  0.0306,\n",
       "                       0.2013, -0.0151,  0.0701,  0.0252, -0.0285,  0.1429,  0.0421,  0.1061,\n",
       "                       0.0306,  0.2221,  0.0857,  0.1527, -0.1146,  0.0997,  0.1591,  0.0149,\n",
       "                       0.1716, -0.0907,  0.0571,  0.0613, -0.0526,  0.0467,  0.0301,  0.0120,\n",
       "                      -0.1324, -0.0965, -0.0177,  0.1689,  0.0762, -0.0452,  0.0952,  0.0411,\n",
       "                      -0.1073, -0.1818,  0.0592, -0.1198,  0.0707,  0.1467,  0.0993,  0.1353,\n",
       "                      -0.0692,  0.0678,  0.1212, -0.0787, -0.1852, -0.0475, -0.1223,  0.1063,\n",
       "                       0.1619,  0.0105,  0.0598,  0.0455, -0.0771, -0.1274, -0.1770, -0.2023])),\n",
       "             ('output.weight',\n",
       "              tensor([[ 0.0540,  0.0359, -0.0223,  ...,  0.0305,  0.0438,  0.0012],\n",
       "                      [-0.0591,  0.0558, -0.0190,  ..., -0.0085,  0.0031, -0.0421],\n",
       "                      [ 0.0117, -0.0208, -0.0240,  ...,  0.0457, -0.0420, -0.0358],\n",
       "                      ...,\n",
       "                      [ 0.0320, -0.0384, -0.0405,  ..., -0.0325, -0.0267,  0.0523],\n",
       "                      [-0.0450, -0.0368,  0.0489,  ...,  0.0298,  0.0595,  0.0321],\n",
       "                      [ 0.0060, -0.0011,  0.0260,  ..., -0.0574, -0.0122, -0.0157]])),\n",
       "             ('output.bias',\n",
       "              tensor([ 0.0117, -0.0556, -0.0097,  0.0028,  0.0034, -0.0351,  0.0431,  0.0414,\n",
       "                      -0.0224,  0.0579]))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)\n",
    "        self.output = nn.Linear(256, 10)\n",
    "    def forward(self, x):\n",
    "        return self.output(F.relu(self.hidden(x)))\n",
    "\n",
    "net = MLP()\n",
    "X = torch.randn(size=(2, 20))\n",
    "Y = net(X)\n",
    "net.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将模型的参数存储在一个叫做\"MLP.params\"的文件中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'data/MLP.params')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了恢复模型，我们实例化原始多层感知机模型的一个备份。我们这里不需要随机初始化模型参数，而是直接读取文件中存储的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (hidden): Linear(in_features=20, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clone = MLP()\n",
    "clone.load_state_dict(torch.load('data/MLP.params'))\n",
    "clone.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于两个模型具有相同的模型参数，在输入相同的X时，两个实例的计算结果应该相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_clone = clone(X)\n",
    "Y_clone == Y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('PyTorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b554925806eef130624e7c02d418c95e40e218f01fd96ae3d2f72202affcda0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
