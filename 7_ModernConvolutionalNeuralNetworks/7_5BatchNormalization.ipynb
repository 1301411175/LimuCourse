{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 批量规范化"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.5.1 训练深层网络"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 首先，数据预处理的方式通常会对结果产生巨大影响。回想我们在应用多层感知机来预测房价的例子。使用真实数据前，我们的第一不是标准化输入特征，使其平均值为0方差为1.直观的说，这种标准化可以很好地与我们的优化器配合使用，因为它可以**将参数的量级进行统一**\n",
        "- 第二，对于典型的多层感知机或卷积神经网络。当我们训练时中间层的变量可能具有更广的变化范围。批量规范化的发明者非正式地提出，这些变量分布阻碍网络的收敛。直观的说，我们可能会想，如果一个层的可变值是另一个层的100倍，这可能需要对学习率进行补偿。\n",
        "- 第三，更深层的网络很复杂，容易过拟合。这意味着正则化变得更重要。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "批量规范化应用于单个可选层（也可以应用所有层），其原理如下：在每次训练迭代中，我们首先规范化输入，即通过减去其均值并除以其标准差，其中两者均基于当前小批量处理。接下来，我们应用比例系数和比例偏移。这是由于这个基于*批量统计*的*标准化*，才有了批量标准化的名称。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\n",
        "BN(X) = \\gamma * \\frac{x-\\check{\\mu}}{\\check{\\sigma}} + \\beta\n",
        "$$\n",
        "在式中，$\\check{\\mu}$是小批量的样本均值，$\\check{\\sigma}$是小批量的样本标准差。应用标准化后，生成的小批量的平均值为0单位方差为1.由于单位方差(与其他一些魔法参数)是一个主观选择，因此我们通常包含*拉伸参数*$\\Gamma$和*偏移参数*$\\Beta$，它们的形状与$x$相同，而且是科学的参数。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.5.2 批量规范化层\n",
        "由于批量规范化层在完整的小批量上运行，因此我们不能像以前在引入其它层时那样忽略批量大小。我们下面讨论这两种情况：全连接层和卷积层，它们的批量规范化实现略有不同。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 7.5.2.1 全连接层\n",
        "通常我们将批量规范层置于全连接层中的仿射变换和激活函数之间。设全连接层的输入为$x$，权重参数和偏置参数为$W$和$b$，激活函数为$\\phi$,批量规范化的运算符为$BN$。那么使用批量规范化的全连接层的输出的计算详情如下：\n",
        "$$\n",
        "h = \\phi(BN(Wx+b))\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 7.5.2.2 卷积层\n",
        "对于卷积层，我们可以在卷积层之后和非线性激活函数之前应用批量规范化。当卷积有多个输出通道时，我们需要对这些通道的“每个”输出执行批量规范化，每个通道都有自己的拉伸(scale)和偏移(shift)参数，这两个参数都是标量。假设我们的小批量样本包含$m$个样本，每个通道卷积的输出具有高度$p$和宽度$q$。那么对于卷积层，我们在每个输出通道的$m·p·q$个元素上同时执行每个批量规范化。因此，在计算平均值和方差时，我们会收集所有空间位置的值然后在给定通道内应用相同的均值和方差，以便在每个空间位置对值进行规范化。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 7.5.2.3 预测过程中的批量规范化\n",
        "批量规范化在训练模式和预测模式下的行为通常不同。首先，将训练好的模型用于预测时，我们不再需要样本均值中的噪声以及在微批次上估计每个小批次产生的样本方差了。其次，我们可能需要使用我们的模型对逐个样本进行预测。一种常用的方法是通过移动平均估算整个训练数据集的样本均值和方差，并在预测时使用它们确定得到的输出。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zi8jT1xAP22"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from d2l import torch as d2l\n",
        "torch.cuda.device_count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B50QZeJzCiXA"
      },
      "source": [
        "从零实现"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BusDkd5-B-Mo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l\n",
        "torch.cuda.device_count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NfqUI3QCfzk"
      },
      "outputs": [],
      "source": [
        "def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
        "  # 通过is_grad_enabled来判断当前模式是训练模式还是预测模式\n",
        "  if not torch.is_grad_enabled():\n",
        "    # 如果是预测模式下，直接使用传入的移动平均所得的均值和方差\n",
        "    X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\n",
        "  else:\n",
        "    assert len(X.shape) in (2, 4)\n",
        "    if len(X.shape) == 2:\n",
        "      # 使用全连接层的情况，计算特征维上的均值和方差\n",
        "      mean = X.mean(dim=0)\n",
        "      var = ((X - mean) ** 2).mean(dim=0)\n",
        "    else:\n",
        "      # 使用二维卷积层的情况，计算通道维上(axis=1)的均值和方差\n",
        "      # 这里我们需要保持x的形状做广播操作\n",
        "      mean = X.mean(dim=(0, 2, 3), keepdim=True)\n",
        "      var = ((X - mean) ** 2).mean(dim = (0, 2, 3), keepdim=True)\n",
        "    # 训练模式下，用当前的均值和方差做标准化\n",
        "    X_hat = (X - mean) / torch.sqrt(var + eps)\n",
        "    # 更新移动平均值和方差\n",
        "    moving_mean = momentum * moving_mean + (1 - momentum) * mean\n",
        "    moving_var = momentum * moving_var + (1 - momentum) * var\n",
        "  Y = gamma * X_hat + beta  # 缩放和移位\n",
        "  return Y, moving_mean.data, moving_var.data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2b4ilFxH4Tc"
      },
      "source": [
        "创建一个正确的`BatchNorm`图层"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOCPzkhsCqxA"
      },
      "outputs": [],
      "source": [
        "class BatchNorm(nn.Module):\n",
        "  def __init__(self, num_features, num_dims):\n",
        "    super().__init__()\n",
        "    if num_dims == 2:\n",
        "      shape = (1, num_features)\n",
        "    else:\n",
        "      shape = (1, num_features, 1, 1)\n",
        "    self.gamma = nn.Parameter(torch.ones(shape))\n",
        "    self.beta = nn.Parameter(torch.zeros(shape))\n",
        "    self.moving_mean = torch.zeros(shape)\n",
        "    self.moving_var = torch.zeros(shape)\n",
        "\n",
        "  def forward(self, X):\n",
        "    if self.moving_mean.device != X.device:\n",
        "      self.moving_mean = self.moving_mean.to(X.device)\n",
        "      self.moving_var = self.moving_var.to(X.device)\n",
        "    Y, self.moving_mean, self.moving_var = batch_norm(\n",
        "        X, self.gamma, self.beta, self.moving_mean, self.moving_var, eps=1e-5, momentum=0.9)\n",
        "    return Y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onZ3MQcNLQ6E"
      },
      "source": [
        "应用`BatchNorm`于LeNet模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNKyBTuPLXa9"
      },
      "outputs": [],
      "source": [
        "net = nn.Sequential(\n",
        "    nn.Conv2d(1, 6, kernel_size=5), BatchNorm(6, num_dims=4),\n",
        "    nn.Sigmoid(), nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "    nn.Conv2d(6, 16, kernel_size=5), BatchNorm(16, num_dims=4),\n",
        "    nn.Sigmoid(), nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "    nn.Flatten(), nn.Linear(16*4*4, 120),\n",
        "    BatchNorm(120, num_dims=2), nn.Sigmoid(),\n",
        "    nn.Linear(120, 84), BatchNorm(84, num_dims=2),\n",
        "    nn.Sigmoid(), nn.Linear(84, 10)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "De8lMmgBM4Z8"
      },
      "source": [
        "在`Fashion-MNIST`数据集上训练网络"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y38_wEb_M9Nm"
      },
      "outputs": [],
      "source": [
        "lr, num_epochs, batch_size = 1.0, 10, 256\n",
        "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
        "d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmHwXm4SO84X"
      },
      "source": [
        "拉伸参数`gamma`及偏移参数`beta`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1caKjRtPPDWO"
      },
      "outputs": [],
      "source": [
        "net[1].gamma.reshape((-1,)), net[1].beta.reshape((-1,))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-rEgvdyPod7"
      },
      "source": [
        "简洁实现"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifEowzquPsfb"
      },
      "outputs": [],
      "source": [
        "net = nn.Sequential(\n",
        "    nn.Conv2d(1, 6, kernel_size=5), nn.BatchNorm2d(6),\n",
        "    nn.Sigmoid(), nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "    nn.Conv2d(6, 16, kernel_size=5), nn.BatchNorm2d(16),\n",
        "    nn.Sigmoid(), nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "    nn.Flatten(), nn.Linear(16*4*4, 120),\n",
        "    nn.BatchNorm2d(120), nn.Sigmoid(),\n",
        "    nn.Linear(120, 84), nn.BatchNorm2d(84),\n",
        "    nn.Sigmoid(), nn.Linear(84, 10)\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.6 ('pytorch')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "7a934d2d03b85ef932848c4d2ee0154df946ef0a94b3a38055cf6cd3d15e3ba5"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
